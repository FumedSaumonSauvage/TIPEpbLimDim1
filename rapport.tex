
% Le template fourni a été utilisé pour rédiger cet article.


\documentclass[12pt]{article}
\usepackage[utf8]{inputenc} 
%\usepackage[francais]{babel}

\usepackage{breqn}
%Dimensions
\usepackage{geometry }
\geometry{left=2cm,right=2cm,top=3.3cm,bottom=3.3cm}

% AMS packages:
\usepackage{amsmath, amsthm, amsfonts}

% Theoremess
%-----------------------------------------------------------------


\newtheorem{theoreme}{Th{\'e}or{\`e}me}[section]
\newtheorem{theoremedefinition}[theoreme]{Th{\'e}or{\`e}me et d{\'e}finition}
\newtheorem{definitionproposition}[theoreme]{Définition et Proposition}
\newcommand{\theoremeautorefname}{Th{\'e}or{\`e}me}
\newtheorem{proposition}[theoreme]{Proposition}%[section]
\newcommand{\propositionautorefname}{Proposition}
\newtheorem{lemme}[theoreme]{Lemme}%[section]
\newcommand{\lemmeautorefname}{Lemme}
\newtheorem{corollaire}[theoreme]{Corollaire}%[section]
\newcommand{\corollaireautorefname}{Corollaire}
\newtheorem{definition}[theoreme]{D\'efinition} %[section]
\newcommand{\definitionautorefname}{D\'efinition}
\newtheorem{definitions}[theoreme]{D\'efinitions}%[section]
\newtheorem{exemple}[theoreme]{Exemple}%[section]
\newtheorem{exemples}[theoreme]{Exemples}%[section]
\newtheorem{remarque}[theoreme]{Remarque}%[section]
\newcommand{\remarqueautorefname}{Remarque}
\newtheorem{remarques}[theoreme]{Remarques}%[section]
\newtheorem{probleme}[theoreme]{Probl{\`e}me}%[section]
\newtheorem{exercice}[theoreme]{Exercice}%[section]


% Shortcuts.
% One can define new commands to shorten frequently used
% constructions. As an example, this defines the R and Z used
% for the real and integer numbers.
%-----------------------------------------------------------------
\newcommand{\R}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\espace}{\hspace{5mm}}

% Similarly, one can define commands that take arguments. In this
% example we define a command for the absolute value.
% -----------------------------------------------------------------
\newcommand{\abs}[1]{\left\vert#1\right\vert}

% Operators
% New operators must defined as such to have them typeset
% correctly. As an example we define the Jacobian:
% -----------------------------------------------------------------
\DeclareMathOperator{\Jac}{Jac}

%-----------------------------------------------------------------
\title{Problèmes aux limites en dimension 1} % votre titre
\author{Simon Hergott} % votre Prénom Nom


\begin{document}
	\maketitle
	
\section{Introduction au problème}

\quad Cet article aura pour but de résumer les résultats et applications concernant les problèmes aux limites en dimension 1. Nous reviendrons sur la méthode des différences finies, fondant l'approximation des solutions des problèmes aux limites, et nous traiterons de plus quelques applications les plus courantes pour cette catégorie de problèmes. Éventuellement, nous verrons brièvement les méthodes de résolution de ces problèmes en dimension 2.
Rappellons d'abord l'énoncé du modèle du problème aux limites en dimension 1:
\begin{equation}\label{probleme}
-u''(x) = f(x) \hspace{1cm} \forall x \in ]0,1[ 
\end{equation}
\begin{equation}\label{limites}
u(0) = u(1) = 0
\end{equation}
\quad Ce problème est donc simplement caractérisé par l'équation \eqref{probleme} munie des conditions limites \eqref{limites}. Une partie importante lors de la résolution de ce problème est donc de trouver les valeurs propres et les fonctions propres (fonctions ne subissant qu'une transformation scalaire lors de leur utilisation comme solution) de l'équation différentielle
\begin{equation}
	X'' + \alpha X = 0 \hspace{1cm} \forall X \in ]0,1[ , \forall \alpha \in \mathbb R*
\end{equation}

\quad Nous pouvons alors écrire les solutions de \eqref{probleme} comme les fonctions de la forme
\begin{equation}
\label{solutionsNaives}
	u\relax(x) = c_1 + c_2 x - \int_0^x F(s) ds \espace c_1 , c_2 \in \R
\end{equation}
selon le théorème fondamental de l'Analyse, avec 
\begin{equation}
	F\relax(s) = \int_0^s f(t) dt
\end{equation}
\quad Nous verrons que l'on peut écrire $u$ sous la forme
\begin{equation}
	u\relax(x) = \int_0^1 G\relax(x, s) f\relax(s) ds \espace x \in [0, 1]
\end{equation}
ce qui nous permettra d'introduire avec $G$ le concept de \emph{Fonction de Greene}.

Les applications du problème aux limites en dimension 1 sont multiples, et relativement nombreuses dans la physique où sa résolution permet la simulation de plusieurs phénomènes tels que la conduction de la chaleur, et les défomrations élastiques.



\section{Méthode des différences finies}

\quad Développée en 1715 par Brook Taylor dans son ouvrage \emph{Methodus incrementorum directa et inversa}, la méthode des diffrences finies est un procédé courant utilisé pour approcher la solution d' équations différentielles. En résumé, on établit une grille de points généralement uniforme sur l'espace de recherche pour discrétiser le problème (le réduire à un nombre fini de pas), puis on réduit la distance entre les points pour approcher au maximum la solution.
\quad Formellement, on utilise la formule de Taylor pour discrétiser les différentielles n-ièmes : on peut alors choisir la formule de Taylor-Young, ou la formule de Taylor avec reste intégral pour évaluer les erreurs (la discrétisation induit une approximation, qui engendre des erreurs).
Dans le cas de Taylor-Young simple (on appellera ici de cette manière la formule de Taylor sans reste intégral), on a:
$$
f\relax(x_0 + h) = f\relax(x_0) + \sum_{\substack{0<i\le n}}\frac{f^{(i)}\relax(x_0)}{i!}h^i
$$ 

\quad La méthode des différences finies sert à approximer la dérivée de certaines fonctions par des valeurs numériques, qui ne peut parfois pas être calculées par des méthodes formelles classiques. En effet, généralement les fonctions ne peuvent pas être formulées analytiquement ce qui rend le calcul de leurs dérivées impossible.
\\
\quad Pour appliquer la méthode des différences finies, on se place sur un segment $[0, \alpha]$ (on verra plus tard que dans le cas du problème aux limites en dimension 1, $\alpha = 1$)sur lequel on ajoute un pas de discrétisation $h$ et une suite de points $x_n | n\in [0, N]$ avec $Nh = \alpha$. Ici, le pas est uniforme mais il peut tout à fait être défini non uniformément sur chaque $x_i$ pour tout $i \in [0, N-1]$ comme $h_i$, avec $\sum_{\substack{0 \le i< N}}h_i = \alpha$. On appellera $x_n$ la \emph{grille}.
\\
\quad Posons une fonction régulière $u$ telle que: $u : [0, \alpha] \longrightarrow \R$, et on appellera $u_n$ l'évaluation de $u$ en chaque point $x_n$ de la grille. \\
On appelle \emph{une différence finie à p points} une combinaison linéaire de $p$ $u_n$, servant à approximer au point $x_n$ les dérivées de $u$. Considérons $Du$ une différence finie, on dira qu'elle \emph{approche $u^{(l)}\relax(x_n)$ à l'ordre q} si :
\begin{equation}
\exists C>0, h_0 >0 \hspace{5mm}|\hspace{5mm} \forall h \in [0, h_0] \hspace{5mm} ||Du - u^{(l)}\relax(x_n)|| \le Ch^q
\end{equation}


%différences finies : voir alfredo quarteroni page 360



\quad Cette méthode permet donc de décomposer les équations différentielles ordinaires en un système d'équations linéaires, solvable en utilisant l'algèbre linéaire.

\subsection{Un exemple : la méthode d'Euler}

\quad Nous nous focaliserons maintenant sur un exemple pour définir la stabilité des méthodes numériques en général : nous utiliserons la méthode d'Euler, aussi appellée \emph{méthode de la tangente}.  Pour cela, nous poserons le problème à valeur initiale
\begin{equation} \label{problemevaleurinitiale}
\frac{dy}{dt} = f(t,y)
\end{equation}
avec la condition initiale
\begin{equation} \label{condInitPVI}
y(t_0) = y_0
\end{equation}
Nous pouvons remarque que ce problème ressemble fortement au problème aux limites, sauf qu'ici il n'y en a qu'une. Dans Méthodes Numériques d'Alfio Quarteroni, cet exemple est utilisé en donnant $\frac{dy}{dt} = 0$, la résolution reste la même.
\quad Nous supposerons ici que les fonctions $f$ et $y$ sont continues dans le compact contenant le point $(t_0, y_0)$ Nous savons alors qu'il existe une solution ans un certain intervalle autour de $t_0$.  Nous supposerons la la solution est unique, en partant du principe que la fonction $f$ est linéaire sans quoi l'intervalle pourrait être difficile à déterminer. 

\quad La méthode d'Euler est relativement simple : si on réécrit l'équation \eqref{problemevaleurinitiale} au point $t_n$ (toujours dans le compact de définition), nous pouvons arriver à la forme
\begin{equation}
\frac{d\phi}{dt}(t_n) = f(t_n, \phi(t_n))
\end{equation}
qui nous permet de la réécrire par le quotient de la différence aux points $t_{n+1}$ et $t_n$:
\begin{equation}
\frac{\phi (t_{n+1}) - \phi(t_n)}{t_{n+1} - t_n} \approx f(t_n, \phi (t_n))
\end{equation}
Enfin, en remplaçant $\phi$ par $y$ et en notant $y(t_n) = y_n$, nous pouvons écrire
\begin{equation}
y_{n+1} = y_n + f(t_n, y_n)(t_{n+1} - t_n) \espace \forall n \in \N
\end{equation}
qui, en supposant le pas uniforme et de valeur $h$ se simplifie en
\begin{equation} \label{eulerSimple}
y_{n+1} = y_n + h*f_n \espace \forall n \in \N
\end{equation}
La méthode d'Euler consiste à calculer l'équation \eqref{eulerSimple} en boucle en utilisant le résultat de l'étape précédente pour la nouvelle évaluation. L'algorithme est très simple, et nous pouvons dès à présent constater qu'il est extrêmement important d'éviter l'amplification d'erreurs d'étape en étape, en évaluant leur propagation.
Afin de comprendre les erreurs apparaissant lors de l'utilisation de méthodes numériques d'approximation, nous allons voir quelques méthodes alternatives pour voir la méthode d'Euler sous un autre angle.
Plusieurs méthodes sont possibles pour y parvenir, l'une d'entre elles est d'écrire le problème \eqref{problemevaleurinitiale} sous la forme d'une intégrale.En supposant $y = \phi(t)$ la solution de notre problème en satisfant la condition \eqref{condInitPVI}, nous avons
\begin{equation}
\int_{t_n}^{t_{n+1}} \phi ' (t) dt = \int_{t_n}^{t_{n+1}} f(t, \phi(t)) dt
\end{equation}
ce qui revient à dire 
\begin{equation} \label{formeJolie}
\phi(t_{n+1}) = \phi(t_n) + \int_{t_n}^{t_{n+1}} f(t, \phi(t)) dt
\end{equation}
L'intégrale de l'équation ci dessus \eqref{formeJolie} peut être représentée graphiquement comme l'aire sous la courbe de $f$ entre $t_n$ et $t_{n+1}$. En approwimant $f$ par sa fonction discrète en posant $f(t, \phi(t)) \approx f(t_n, \phi(t_n))$ et en gardant l'hypothèse que nous sommes sur une \emph{grille} (si ce concept n'a pas encore été défini, il le sera tout à l'heure) de pas uniforme $h$, nous avons alors:
\begin{equation}
\phi(t_{n+1}) \approx \phi(t_n) + hf(t_n, \phi(t_n))
\end{equation}

\quad Une autre approche consiste à supposer que la solution $\phi(t)$ est développaple au sens de Taylor (en série de Taylor) en $t_n$:
\begin{equation}
\phi (t_n + h) = \phi (t_n) + \phi'(t–n)h + \phi''(t_n) \frac{h^2}{2!} + ...
\end{equation}
Cette expression suppose que le développement de Taylor comprend plus que 2 termes, sinon nous aurions retrouvé une écriture précédente de la formule d'Euler.

\quad L'utilisation de méthodes numériques telles que la formule d'Euler engendre des erreurs, qui doivent être identifiées et dont l'importance doit être vérifiée avant de pouvoir utiliser la solution approximée comme satisfaisante. En effet, un problème se pose pour la \emph{convergence} de la solution : lorsque le pas $h$ (en supposant qu'il soit uniforme, sinon considérer la distance entre les points de la grille) tend vers 0, est ce que al distance entre les solutions approximéees $y_n$ en les points $t_n$ tend vers 0? Et approchent-elles les véritables solutions du problème?
Même en répondant à ces questions, nous pouvons encore nous demander avec quelle vitesse les solutions convergent-elles ; reformulé, à quel niveau de précision sur $h$ faut-il aller afin de garantir une certaine marge d'erreur maximum? Certains effets de bord peuvent aussi ne pas être évidents : la diminution de la taille du pas afin de garantir une meilleure précision pourrait causer de plus grandes erreurs, qui annuleraient tout bénéfice.

Il y a un certain nombre de sources fondamentales d'erreurs dans l'approximation numérique d'un problème de valeur initiale (ce qui s'étend aux problèmes aux limites):
\begin{enumerate}
 	\item La formule est une approximation, donc source d'erreur. Dans le cas de la méthode d'Euler, la solution est approximée par une ligne droite en lieu et place de la courbe.
	\item Les données utilisées sont souvent elle mêmes des approximations (voir à nouveau la méthode d'Euler)
	\item Éventuellement, dans le cas d'un calcul sur ordinateur la précision pour chaque variable est finie (la mémoire ne peut stocker de valeurs analogiques)
\end{enumerate}

En supposant que lors d'un calcul sur ordinateur la précision soit infinie (nous venons de vérifier que ce n'est pas vrai), nous pouvons poser $E_n$ l'erreur d'approximation en chaque point $t_n$ par:
\begin{equation}
E_n = \phi(t_n) - u_n
\end{equation}
Cette erreur ne repose que sur les deux premiers points de la liste, elle est parfois appellée \emph{erreur globale de troncature}. \\
\quad En revanche, en relâchant l'hypothèse d'avoir des ordinateurs parfait, nous devons faire face à une erreur supplémentaire,  \emph{l'erreur d'arrondi} qui intervient dès lors que l'on calcule avec une arithmétique à \emph{précision finie}. Cette erreur d'arrondi en un point $t_n$ s'exprime comme
\begin{equation}
R_n = y_n - Y_n
\end{equation}
avecc $Y_n$ la valeur calculée par la machine en utilisant la méthode numérique. \\
En ajoutant ces erreurs, nous pouvons constater que l'erreur totale est la somme de la valeur absolue des erreurs d'approximation, et des erreurs d'arrondi.\\

\subsection{Consistance}
\quad Étant donné ces erreurs, nous pouvons alors évaluer la \emph{consistance} d'un problème: les méthodes génériques pour approcher la solution de \eqref{problemevaleurinitiale} (en considérant $\frac{dy}{dt} = 0$ pour simplifier) sont en général toutes constituées d'une suite de problèmes approchés
\begin{equation}
f_n(t_n, y_n) = 0
\end{equation}

Nous supposerons que le problème est \emph{bien posé} (que sa solution existe, soit unique et dépende continûment des données), sans quoi il devient difficile de montrer sa consistance. Nous verrrons plus tard que les problèmes aux limites considérés sont eux aussi bien posés, cette hypothèse n'est donc pas gênante ici.\\
\quad Nous pouvons alors définir la notion de consistance d'une méthode numérique donnée comme:
\begin{equation} \label{defConsistance}
n \rightarrow \infty \espace \Rightarrow \espace f_n(t_n, y) - f(t,y) \rightarrow 0
\end{equation}
Intuitivement, elle correspond à la quantité d'erreur commise par le schéma numérique, au temps $t_n$ et doit tendre vers une valeur infiniment petite.
Si cette relation est vraie, la méthode numérique est dite consistante. Sinon, elle ne l'est pas. De plus, une méthode numérique est \emph{fortement consistante} si
\begin{equation}
\forall n \in \N \espace  f_n(t, y) - f(t,y) = 0
\end{equation}
\\
\quad Nous retrouvons alors la notion de stabilité: une méthode numérique sera considérée \emph{bien posée} ou \emph {stable} si pour tout $n$ il existe une solution $t_n$ \emph{unique} correspondant à la donnée $y_n$, et que $t_n$ dépend des $y_n$ continûment.


(ici arretés page 38 de quarteroni)



\section{Résolution du problème en dimension 1}

\quad Revenons à l'introduction de cet article : nous avions vu que pour $u \in C^2 [0, 1]$ satisfaisant l'équation \eqref{probleme}, on avait $u$ de la forme \eqref{solutionsNaives}  Pour retrouver la fonction de Greene, nous devons intégrer par parties $\int_0^x F\relax(s) ds$ : 
\begin{equation}
\int_0^x F\relax(s) ds = [sF\relax(s)]_0^x - \int_0^x sF'\relax(s) ds = \int_0^x(x-s)f\relax(s)ds
\end{equation}

D'après \eqref{limites}, les constantes $c_1$ et $c_2$ sont respectivement égales à $0$ et $\int_0^1(1-s)f\relax(s)ds$.  On peut alors écrire $u$ sous la forme
\begin{equation} \label{introGreene}
u\relax(x) = x \int_0^1(1-s)f\relax(s)ds - \int_0^x (x-s)f\relax(s)ds = \int_0^1 G\relax(x, s) f\relax(s) ds
\end{equation}
avec 
\begin{equation}
G\relax(x, s) = \left\{
    \begin{array}{ll}
        s(1-x) \espace s\in [0,x] \\
        x(1-s) \espace s\in [x, 1]
    \end{array}
\right.
\end{equation}
On remarquera qu'on a continuité de la fonction $G$ pour $s = x$, et que G est une fonction \emph{affine} de $x$ à $s$ fixé, et de $s$ à $x$ fixé. Elle se nomme \emph{Fonction de Greene} pour le problème aux limites défini par \eqref{probleme} et \eqref{limites}.

\subsection{L'histoire avec Greene}
\quad La fonction de Greene est continue et symétrique sur $[0,1]^2$, ainsi que positive non strictement ($G(x,s) = 0 \Longleftrightarrow x = 0 \lor s = 0$). Son existence dans le problème générique \eqref{probleme} (nous verrons plus tard des applications précises du problème comme la déformation d'une corde élastique) est assurée par sa définition sur tout l'intervalle entre les limites (dans notre cas, \eqref{limites} mais on pourrait fixer d'autres limites). \\
\quad On voit alors que lorsque $G$ existe et qu'elle est connue formellement, on peut écrire explicitement les soutions du problème aux limites dans une forme très simple.  Un des principaux avantages de la représentation \eqref{introGreene} des solutions du problème est qu'elle élimine la dépendance au terme $f(s)$, qui n'est dépendant que de l'équation différentielle en \eqref{probleme} et des limites qui nous sont imposées : une fois que l'on aura déterminé $G$, les solutions seront connues selon $f(s)$ pour peu pqu'on puisse écrire les solutions sous la forme \eqref{introGreene}. Nous verrons dans les applications que cette forme n'est pas forcément admissible sous toutes les conditions. \\
\quad De plus, la forme intégrale \eqref{introGreene} est bien plus propice à l'analyse numérique que l'équation différentielle \eqref{probleme}, ce qui rend le traitement par ordinateur bien plus simple et efficace. Les motivations principales sont donc de passer d'une recherche de $u$ à une recherche de $G$ : il faut alors trouver un moyen d'exprimer $G$ sans avoir à résoudre l'équation \eqref{probleme}.\\
Nous pouvons alors lister quelques propriétés de la fonction de Greene, qui nous seront utiles par la suite dans la résolution du problème:
\begin{enumerate}
  \item $G$ satisfait l'équation homogène de  \eqref{probleme}:
	\begin{equation}
		G'' = 0
	\end{equation}
	sur les intervalles $0 \le s < x$ et $x < s \le L$ avec dans notre cas L = 1. Généralement, on a bien continuité en $s = x$, nous le prouverons dans le cas du problème de l'élasticité d'une corde. (ou pas? Remettre la démo éventuellement)
  \item $G(x, 0) = G(x, L)$, elle satisfait donc les conditions limites \eqref{limites}.
  \item $G(x,s) = G(s, x)$, G est symétrique sur ses arguments.
\end{enumerate}

A partir de ces propriétés, nous pouvons commencer la résolution du problème aux limites en supposant l'existence d'une fonction de Greene $G$. Si cette supposition est valide, alors nous pouvons retrouver \eqref{introGreene} depuis \eqref {probleme}, ainsi que ses propriétés listées précédemment.  En effet,  d'après \eqref{probleme}, après une multiplication des deux côtés par $G$ nous pouvons intégrer comme suit:
\begin{equation}
\int_0^1 u'' G(x,s) dx = - \int_0^1 f(x) G(x,s) dx
\end{equation}
avec comme seule supposition la continuité lorsque $s \rightarrow x$. Pour être plus formel, nous pouvons exclure de l'intervalle d'intégration le point $x = s$ et éviter toute intégrale impropre.
\begin{equation} \label{separation}
\int_0^1 u'' G(x,s) dx = \lim_{\epsilon \to s^- } \int_0^\epsilon u'' G(x,s) dx + \lim_{\eta \to s^+ } \int_\eta^1 u'' G(x,s) dx
\end{equation}

En intégrant deux fois par parties les deux intégrales à droite,  nous avons:
\begin{equation}
\int_0^\epsilon u'' G(x,s) dx = [Gu' - G'u]_0^\epsilon + \int_0^\epsilon u G''(x,s) dx
\end{equation}
\begin{equation}
\int_\eta^1 u'' G(x,s) dx = [Gu' - G'u]_0^\epsilon + \int_\eta^1 u G''(x,s) dx
\end{equation}
Or, en choisissant $G(x,s)$ de manière à satisfaire $G'' = 0$ en tant que fonction de x dans les intervalles des intégrales ($[0, \epsilon] \cup [\eta, 1]$), alors les intégrales à droite sont nulles. En ajoutant les termes restants (qu'on avait enlevés afin d'éviter les intégrales impropres), on revient en supposant $G$ symétrique à:
\begin{dmath}
-\int_0^1 f(x)G(x, s) dx = (G(s, s^- ) u'(s^-) - G'(s, s^- ) u(s^-) - G(0, s ) u'(0) + G(1, s ) u'(1) - G(s, s^+ ) u'(s^+) + G'(s, s^+ ) u(s^+))
\end{dmath}

En supposant que $G$ vérifie les conditions limites et que $u$ est continu en $s$,  nous pouvons écrire: 
\begin{equation} \label{dernierRempartDeLEncadre}
\int_0^1 f(x)G(x, s) dx = - u(y) (G'(s, s^+ ) - G'(s, s^- ))  + u'(s)( G(s, s^+ ) - G(s, s^- ))
\end{equation}


%METTRE TOUT CA DANS UN ENCADRÉ
De plus,  $G$ est continue en $x=s$, tandis que $G'$ y est discontinue. Nous aurons besoin de cette proposition pour continuer, nous allons la démontrer. \\
À partir de \eqref{probleme}, nous pouvons utiliser la méthode de la variation des variables pour supposer que, si le problème existe, alors les solutions seront de la forme 
\begin{equation} \label{solutionBasique}
u(x) = A(x) cos(kx) + B(x) sin(kx).
\end{equation}
En dérivant deux fois selon x et en supposant qu'on ait $A'(x) cos (kx) + B'(x) sin(kx) = 0$, on trouve que \eqref{solutionBasique} constitue effectivement une solution, à la condition
\begin{equation} \label{resoudre}
-kA' sin(kx) + kB' cos(kx) = -f(x)
\end{equation}
En résolvant \eqref{resoudre} accompagné de sa condition présupposée, nous pouvons alors exprimer $A'$ et $B'$:
\begin{equation}
A'(x) = \frac{f(x) sin(kx)}{k}
\end{equation}
\begin{equation}
B'(x) = \frac{-f(x) cos(kx)}{k}
\end{equation}

Nous pouvons alors écrire la solution de \eqref{probleme} comme:
\begin{equation}
u(x) = \frac{cos(kx)}{k} \int_{c_1}^x f(s) sin (ks) ds - \frac{sin(kx)}{k} \int_{c_2}^x f(s) cos (ks) ds
\end{equation}
avec $c_1, c_2$ constantes bien choisies pour satisfaire les conditions \eqref{limites}. En utilisant la condition en 0 de \eqref{limites}, on trouve $c_1 = 0$. De même, la condition en 1 implique
\begin{equation}
u(x) = \frac{1}{k} \int_0^x f(s) sin (k(s-x)) ds - \frac{sin(kx)}{k sin(kl)} \int_x^1 f(s) sin(k(s-1) ds
\end{equation}
ce qui nous donne enfin
\begin{dmath}
u(x) =  \int_0^x\frac{sin(ks) sin(k(1-x))}{k sin(k)}ds + \int_x^1 f(s) \frac{sin(kx) sin(k(1-s))}{k sin(kl)} ds = \int_0^1 f(s) G(x,s) ds
\end{dmath}
qui revient au même pour introduire la fonction de Greene, de manière bien détaillée cette fois. On a par la même occasion montré la consistance du problème aux limites (revoir la def!!!!). \\
Finalement, nous aboutissons à une forme de $G$ intéressante:
\begin{equation}
G(x, s) = \frac{sin(ky) sin(k(1-x)}{k sin(k)} \espace s\in [0, x] 
\end{equation}
\begin{equation}
G(x, s) = \frac{sin(kx) sin(k(1-s)}{k sin(k)} \espace s\in [x,1]
\end{equation}

À partir d'ici, nous pouvons vérifier aisément que la fonction $G$ est continue en $s=x$, et que sa dérivée est discontinue en ce même point.
%FIN DE L'ENCADRÉ
\\
Depuis \eqref{dernierRempartDeLEncadre}, nous pouvons alors conclure que
\begin{equation}
u(s) = \int_0^1 f(x)G(x, s) ds
\end{equation}
et nous obtenons alors une représentation désirable de la solution duproblème aux limites par une fonction $G$. Cette fonction vérifie tous les axiomes de la fonction de Greene énoncés auparavant, nous avons donc établi une méthode pour retrouver la solution des problèmes aux limites d'une manière relativement directe. Néanmoins, cette méthode suppose que nous connaissions la fonction $G$.

\subsection{Suite de la resolution : approximation par différences finies}

Sur la grille des points $(x_j)_{j = 0}^n$ de pas uniforme $h$ (on a donc $x_j = jh$) sur $[0,1]$, l'approximation de la solution est une suite finie $(u_j)_{j = 0]}^n$ telle que:
\begin{equation} \label{diffsFiniesProbleme}
	- \frac{u_{j+1} - 2u_j + u_{j-1}}{h^2} = f(x_j) \espace j \in [[1, n-1]]
\end{equation} 
avec $u_i = 0$ aux points $0$ et $n$. On a alors $u_j$ approche $u(x_j)$, la valeur de chaque point de la grille. Pour s'en convaincre, on se réfèrera à ... dans la partie traitent des différences finies centrées, en remplaçant $u''(x)$ par son approximation du 2nd ordre.

On posera $u = <u_1, ..., u_n-1>$ et $f = <f_1, ..., f_n-1>$ vecteurs avec $f_i = f(x_i)$, en utilisant une notation empruntée au C++. Nous pouvons alors voir que \eqref{diffsFiniesProbleme} peut s'écrire comme
\begin{equation} \label{turboCompact}
	A_{df}u = f
\end{equation}
avec $A_{df}$ matrice carrée de différences finies de taille $(n-1)$ définie par
\begin{equation}
	A_{fd} = h^{-2} tridiag_{n-1} (-1, 2, -1)
\end{equation}
Elle est à diagonale dominante par ligne, et définie positive (démontrable).

Alors, l'équation \eqref{turboCompact} n'admet qu'une unique solution.\\ Définissons les M-Matrices comme suit : une M-Matice est une matrice carrée inversible avec tous ses coefficients non sur la diagonale négatifs ou nuls, ainsi que tous les coefficients de son inverse sont positifs ou nuls.
Nous pouvons noter que $A_{df}$ est une M-Matrice, ce qui permet de satisfaire la condition de monotonie de la solution exacte $u(x)$ : nous avons alors $f >0 \Longrightarrow u>0$ (propriété du \emph{maximum discret}).

Le but est maintenant de réécrire \eqref{diffsFiniesProbleme}. Pour cela, nous pouvons considérer $V_h$ ensemble de fonctions discrètes définies sur les $x_j$ points de la grille. Pour tout $v_h$ dans $V_h$, elle est alors définie en tout point de la grille, et $v_j = v_h(x_j)$.\\ On posera de plus $V_h^0 = \{v_h \in V_h | v_0 = v_n = 0\}$.
On pourra définir $L_h$ comme
\begin{equation} \label{notationvj}
	(L_h v_h)(x_j) = - \frac{v_{j+1} - 2v_j + w_{j-1}}{h^2} \espace \espace v_h \in V_h \espace j \in [[1, n-1]]
\end{equation}
On peut alors encore réécrire le problème \eqref{diffsFiniesProbleme} comme:
\begin{equation} \label{diffFiniesAvecL}
	(L_h u_h)(x_j) = f(x_j) \espace j \in [[1, n-1]]
\end{equation}
en cherchant $u_h \in V_h^0$ : on prend en compte les conditions limites.

%analyse de la stabilité
\subsection{Analyse de la stabilité}
Nous allons maintenant analyser la \emph{stabilité} du problème. On peut définir la stabilité par la capacité de l'algorithme de résolution à ne pas amplifier les erreurs, et à produire des résultats cohérents. Ici, vous allons essayer de montrer que la solution renvoyée en appliquant la méthode des différences finies au problème est bornée par une des variables d'entrées.

Pour cela, nous aurons besoin de quelques notions supplémentaires. Nous définirons le \emph{produit scalaire discret} comme
\begin{equation} \label{produitScalaireDiscret}
	(w_h, v_h)_h = h \sum_{k = 0}^n c_k w_k v_k \espace \forall v_h, w_h \in V_h
\end{equation}
avec $c_i= 1 \forall i \in [[1, n-1]]$ et $c_0 = c_n = 1/2$. On peut retrouver le produit scalaire discret par différences finies, avec la \emph{formule composite du trapèze} (voir partie sur les différences finies). On peut alors définir une norme sur $V_h$ par la racine du produit scalaire d'un opérateur de $V_h$ avec lui même.

Nous pouvons alors émettre plusieurs propriétés sur ce produit scalaire. Tout d'abord, il est symétrique et défini positif pour l'opérateur $L_h$ dans $V_h^0$:
\begin{equation}
	(L_h w_h, v_h)_h = (v_h, L_h w_h)_h \espace (L_h v_h, v_h)_h \ge 0 (0 \Leftrightarrow v_h = 0)
\end{equation}
(démonstration possible)

On définit la norme $|||.|||_h$ sur $V_h^0$ par:
\begin{equation}
	|||v_h||| = \left\{ h \sum_{j = 0}^{n-1} ( \frac{v_{j+1} - v_j}{h})^2 \right\} ^{1/2}
 \end{equation}
On peut alors constater que
\begin{equation} \label{normecarrée}
	(L_h v_h)_h = |||v_h|||^2_h \espace \forall v_h \in V^0_h
\end{equation}

De plus, on a:
\begin{equation} \label{majorationNorme}
	||v_h||_h^2 \le \frac{1}{\sqrt{2}} |||v_h||||_h \espace \forall v_h \in V_h^0
\end{equation}
(démonstration possible, on utilise l'inégalité de Minkowski en repartant de la définition des $v_j$)\\
De même, on peut écrire la version discrète de \emph{l'inégalité de poincaré}: en notant $v_h^{(1)}$ la fonction discrète de $v_h$ dans $V_h^0$ prenant ses valeurs sur la grille par $(v_{j+1}-v_j)/h$ avec $j \in [[0, n-1]]$, on peut la voir commme la dérivée discrète de $v_h$.\\
On peut alors écrire l'inégalité comme
\begin{equation}
	||v_h||_h \le \frac{1}{\sqrt{2}} ||v_h^{(1)}||_h \forall v_h \in V_h^0
\end{equation}

À partir de \eqref{majorationNorme}, on peut multiplier chaque équation par \eqref{diffFiniesAvecL} pour avoir
\begin{equation}
	(L_h u_h , u_h )_h = (f, u_h)_h
\end{equation}

En reprenant la notation introduite dans \eqref{notationvj}, on peut définir $f_d$ fonction discrète égale à $f$ en chaque poitnde la grille. On a alors $f_d(x_i) = f(x_i)$.  À partir de \eqref{normecarrée}, on peut trouver
\begin{equation}
|||u_h|||_h^2 \le ||f_d||_h ||u_h||_h
\end{equation}
à partir de l'inégalité de Schwarz.
Alors,  on a 
\begin{equation} \label{preuveStabilité}
||u_h||_h \le \frac{1}{2}||f_d||_h
\end{equation}
Regarder d'où sortent les uh qui trainent par ici!!!!!!!!!!!!!!!!!!!!!!!!! \\
ce qui conclut que la seule solution correspondant à $f_d = 0$ est $u_h = 0$, donc le problème aux différences finies ne possède qu'une seule solution. La stabilité, titre de cette section, est quant à elle donnée par la majoration (ou \emph{borne}, comme on est sur une norme) de la solution par $f_d$ qui est une des données du problème.

\subsection{Consistance}
Nous devons maintenant prouver la convergence du problème aux limites. Nous devrons introduire ici la notion de \emph{consistance} : elle a été définie dans le cadre général par \eqref{defConsistance}. Dans notre cas, si $f \in C^0([0,1])$ et la solution de \eqref{probleme} $u$ est de classe $C^2$ sur ce même segment (dont les bornes correspondent aux cas limites) alors nous pouvons considérer \emph{l'erreur de troncature locale} $\tau_h$ définie par
\begin{equation}\label{erreurTroncatureLocale}
\tau_h(x_j) = (L_h u)(x_j) - f(x_j) \espace j \in [[1,n-1]]
\end{equation}
En développant par Taylor, on a:
\begin{equation}
\tau_h(x_j) = (L_h u)(x_j)  + u''(x) = -\frac{h^2}{24}(u^{(iv))} (\chi_j) + u^{(iv))} (\nu_j))
\end{equation} \label{devTaylorTroncLoc}
avec $\nu_j \in ]x_j, x_{j+1}[ \wedge \chi_j \in ]x_{j-1}, x_j[$.
\\ On peut définir la \emph{norme discrète du maximum} par
\begin{equation} \label{defNormeDiscreteDuMaximum}
|| v_h ||_{h, \infty}  = \max_{0 \le j \le n} |v_h(x_j)|
\end{equation}
ce qui nous permet de déduire de  \eqref{devTaylorTroncLoc} que
\begin{equation}
|| \tau_h ||_{h, \infty} \le \frac{||f''||_\infty}{12} * h^2
\end{equation}
à partir du moment où on a $f \in C^2([0,1])$ dans \eqref{probleme}. De plus, dans notre cas avec les conditions \eqref{limites} nous avons $\lim_{h \rightarrow 0} \tau_{h, \infty} = 0$ : la méthode des différences finies est donc consistante avec notre problème.

\subsection{Convergence}
Cette partie sera laissée de côté dans cette analyse, car elle est relativement complexe et ne présente pas de réel intérêt : le résultat de la convergence reprend les arguments de stabilité et de consistance, et montre que l'erreur de discrétisation est comparable à l'erreur de stabilité.\\
Nous définirons en revanche les fonctions $w_h$ pour toute fonction discrète $g \in V_0^h$, qui nous seront utiles par la suite dans la résolution du problème en une dimension:
\begin{equation} \label{defWK}
w_h = \sum_{k=1}^{n-1} g(x_k) G^k
\end{equation}
où nous posons G fonction de Greene.

Nous laisons le soin au lecteur de voir p 428 d'alfio quarteroni.

\subsection{différences finies pour les problèmes en dimension 1}
Dans cette partie, on utilisera une généralisation du problème aux limites \eqref{probleme} \eqref{limite} en une dimension:
\begin{equation} \label{generalisationLu}
Lu(x) = -(J(u)(x))' + \gamma (x) u(x)  = f(x) \espace \forall x \in ]0,1[
\end{equation}
avec les conditions limites $d_0$ et $d_1$:
\begin{equation}
u(0) = d_0 \espace u(1) = d_1
\end{equation}
en posant $J(u)(x) = \alpha(x) u'(x)$ et $\alpha$, $\gamma \ge 0$, $f$ des fonctions connues et continues sur l'intervalle $[0,1]$. L'hypothèse de continuité est généralement vérifiée dans les applications physiques.  Nous appellerons $J(u)$ le \emph{flux associé à u}, il nous sera utile dans la partie de résolution des applications.

Pour l'application, nous suivrons la méthode décrite dans Alfio Quarteroni : nous reprendrons la grille uniforme précédente des $(x_j)$, et nous créerons une nouvelle grille utilisant les \emph{points milieu} de cette grille: nous définirons les point milieu comme les $x_{j+1/2} = (x_j + x_{j+1}) /2 \espace \forall j \in [[1,n-1]]$. \\
Alors, nous pouvons introduire un nouveau schéma aux différences finies pour approcher \eqref{generalisationLu}: nous cherchons $u_h \in V_h \espace |$
\begin{equation} \label{miseEnFormeTemporaire}
L_h u_h (x_j) = f(x_j) \espace \forall j \in [[1,n-1]]
\end{equation}
avec les conditions limites
\begin{equation}
u_h(x_0) = d_0 \espace \wedge u_h(x_n) = d–1
\end{equation}
en définissant $L_h$ à partir de la fonction $\gamma$ précédente et des $w_k$ de \eqref{defWK} par:
\begin{equation} \label{katto}
	L_h w_h (x_j) = - \frac{J_{j + 1/2}(w_h) - J_{j - 1/2}(w_h)}{h} + \gamma_j w_j  \espace \forall j \in [[1,n-1]]
\end{equation}
On a repris ici la notation habituelle $\gamma_j = \gamma(x_j)$. Nous pouvons alors poser $\alpha_{j+1/2} = \alpha(x_{j+1/2})$, afin de définir les \emph{flux approchés} $J_i \espace \forall i \in [[1/2,n-1/2]]$:
\begin{equation}
J_{j+1/2} (w_h) = \alpha_{j+1/2} * \frac{w_{j+1} - w_j}{h}
\end{equation}

À partir de cette formulation avec des flux approchés, nous pouvons alors remettre en forme le problème \eqref{miseEnFormeTemporaire} ainsi que ses conditions limites avec les flux approchés $J_i$. Pour cela, nous aurons besoin de définir la matrice de différences finies $A_{df}$ comme:
\begin{equation} \label{definitionADF}
A_{fd} = h^{-2} tridiag_{n-1} (a,d,a) + diag_{n-1}(c)
\end{equation}
avec:
\begin{itemize}
  \item $a = -[\alpha_{3/2}, ..., \alpha_{n-3/2}]^T$ (indices de pas 1)
  \item $d = [\alpha_{1/2}+\alpha_{3/2}, ..., \alpha_{n-3/2} +\alpha_{n-1/2}]^T$
  \item $c = [\gamma_1, ..., \gamma_{n-1}]^T$
\end{itemize}
Avec les facteurs de $a$, $d$, $c$ dans $\R$. La démonstration est assez longue et calculatoire, et globalement peu intéressante ici, nous renvoyons à Alfio Quarteroni?.

Nous pouvons dès lors remarquer que la matrice $A_{fd}$ de \eqref{definitionADF} est définie positive, symétrique, et à diagonale strictement dominante (la valeur avsolue de chaque terme sur la diagonale est supérieur non strictement à la somme des valeurs absolues des autres termes de sa ligne) pourvu que $\gamma > 0$.\\
\quad On peut analyser la convergence de ce schéma aux différences finies en reprenant la technique de la Méthode de l'Energie vue précédemment.
Il est possible enfin de définir différentes conditions limites, plus générales que celles définies au début de cette section : une pléthore de ces conditions ont vu le jour dans des applications physiques, nous nous focaliserons principalement sur les conditions dites de \emph{Neumann} et de \emph{Dirichlet}. Ces conditions s'expriment respectivement comme
\begin{equation} \label{limitesNeumannDirichlet}
J(u)(1) = g_1 \espace ; \espace u(0) = d_0
\end{equation}

Nous allons discrétiser la condition de Neumann, en utilisant la \emph{technique du miroir}, qui consiste à ne garder qu'une partie simple d'une fonction $\psi$ et à la développant linéairement:
\\Posons $\psi$ fonction régulière, nous pouvons la développer en sérien de Taylor en $x_n$ comme 
\begin{equation}
\psi_n = \frac{\psi_{n-1/2} + \psi_{n+1/2}}{2} - \frac{h^2}{16}(\psi '' (\eta_n) + \psi '' \zeta_n))
\end{equation}
en choisissant $\eta_n$ dans $[x_{n-1/2}, x_n[$ et $\zeta_n$ dans $]x_n, x_{n+1/2}].$ On peut alors utiliser la condition limite \eqref{limitesNeumannDirichlet},
\begin{equation} \label{pointFantome}
\psi = J(u) \espace \Rightarrow \espace J_{n+1/2}(u_h) = 2g_1 - J_{n-1/2}(u_h)
\end{equation}
Le point $x_{n+1/2} = x_n + h/2$ n'existe pas comme il sort de la grille (on suppose le pas uniforme, et différent de 0) : il est appellé \emph{point fantôme} dans Alfio Quarteroni.  Nous obtenons le flux approché correspondant par prolongation linéaire des deux flux précédents $J_{n-1/2}$ et $J_n$. En $x_n$, l'équation \eqref{katto} se reformule
\begin{equation}
\frac{J_{n-1/2}(u_h) - J_{n+1/2}(u_h)}{h} + \gamma_n u_n = f_n
\end{equation}
Nous pouvons alors reprendre le point fantôme vu dans \eqref{pointFantome} pour que $J_{n+1/2}$ existe (ou pas, finalement...), et on obtient une approximation à l'ordre 2 en développant l'équation précédente:
\begin{equation}
- \alpha_{n-1/2} \frac{u_{n-1}}{h^2} + (\frac{\alpha_{n-1/2}}{h^2} + \frac{\gamma_n}{2} ) u_n = \frac{g_1}{h} + \frac{f_n}{2}
\end{equation}

À partir de cette formule, nous pouvons alors modifier simplement les coefficients de la matrice de \eqref{turboCompact}, ce qui termine l'utilisation des différences finies pour la résolution du problème en dimension 1.


\subsection{Introduction à la formulation intégrale}

Nous utiliseraons dans cette partie les notations d'alfio Quarteroni. Le problème aux limites \eqref{probleme} peut être généralisé comme
\begin{equation} \label{generalisation}
-(\alpha u')'(x) + (\beta u')(x) + (\gamma u)(x) = f(x) \espace \forall u \in ]0,1[
\end{equation}
en vérifiant les conditions limites \eqref{limites} en posant $u(0) = u(1) = 0$, avec $\alpha$, $\beta$, $\gamma$ des fonctions continues sur $]0,1[$, et en supposant l'existence d'une constante $\alpha_0$ telle que $\alpha(x) \ge \alpha_0 > 0 \espace \forall x \in [0,1]$. \\
\quad Nous pouvons alors utiliser une méthode dérivée de la fonction test : posons une fonction $v$ de classe $C^1$ sur $[0,1]$ que l'on choisira en tant que fonction test. Nous pouvons alors multiplier \eqref{generalisation} par $v$ et l'intégrer sur l'intervalle $[0,1]$:
\begin{equation} \label{patéIntegrale}
\int_0^1 \alpha u' v' dx + \int_0^1 \beta u' v dx + \int_0^1 \gamma u v dx = \int_0^1 fv dx + [\alpha u' v]_0^1
\end{equation}
après une intégration par parties sur le premier terme. En appliquant les conditions limites sur $v$ (on oblige $v$ à être nulle en 0 et en 1), on a alors $[\alpha u' v]_0^1 = 0$ ce qui nous donne
\begin{equation}
\int_0^1 \alpha u' v' dx + \int_0^1 \beta u' v dx + \int_0^1 \gamma u v dx = \int_0^1 fv dx 
\end{equation}
Voir si on garde le truc avant, ca sert peu!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

On posera $V$ l'espace des fonctions test telles $v$, en prenant compte les conditions limites : cet espace contient des fonctions continues, s'annulant en 0 et en 1, et de dérivée continue par morceaux. C'est alors un espace vectoriel, que l'on peut aussi noter 
\begin{equation}
H_0^1(]0,1[) = \{v \in L^2(]0,1[) \espace | \espace v' \in L^2(]0,1[) \wedge v(0) = v(1) = 0\}
\end{equation}
en notant $v'$ la \emph{dérivée au sens des distributions}, dont la définition sera donnée dans la section suivante (bases des distributions).

Nous venons de montrer que si une fonction $u$ de $C^2([0,1])$ satisfait la généralisation \eqref{generalisation} du problème, alors $u$ est aussi la solution du problème
\begin{equation} \label{problemeSecondaire}
a(u,v) = (f,v) \espace u \in V \espace \forall v \in V
\end{equation}
où on définit $(f,v) = \int_0^1 fv dx$ comme le produit scalaire de $L^2(]0,1[)$, avec de plus
\begin{equation}
a(u,v) = \int_0^1 \alpha u' v' dx + \int_0^1 \beta u' v dx + \int_0^1 \gamma u v dx
\end{equation}
forme bilinéaire par rapport à $(u,v)$.

On appellera \emph{formulation faible} le problème \eqref{problemeSecondaire} : cette formulation est plus généaliste, car ellle permet d'étudier des cas où $u$ n'est pas de classe $C^2$, car les dérivées sont toutes simples. Cette formulation permet de décrire par exemple le problème de la déformation d'une corde élastique, qui sera étudié par la suite.

En cas de non homogénité des conditions limites dans \eqref{generalisation} (ce qui arrive dans le cas général), il est toujours possible de se ramener à la forme \eqref{problemeSecondaire} en utlisant des \emph{conditions limites de Neumann} et une interpolation des extrémités par une fonction affine (détails dans quarteroni, page 433).

\subsection{Bases des distributions}

Cette section est dédiée à la présentation des bases de la théorie des distributions, qui nous sera utile ors de l'étude des propriétés de la méthode de Galerkin. En résumé, la théorie des distributions est un outil servant à généraliser la notion de dérivée d'une fonction, afin de pouvoir résoudre certaines équations différentielles (par exemple, un problème aux limites en dimension 1). Les distributions utilisent une méthode différente pour évaluer une fonction : au lieu d'évaluer la fonction en un point, on évalue la fonction sur un voisinage du point tout en effectuant une moyenne (posiblement pondérée) sur ces points afin de réduire le poids des discontinuité dans le résultat.

Supposons $X$ un espace vectoriel normé complet (c'est à dire un \emph{espace de Banach}). Définissons une \emph{forme} $T: X \rightarrow \R$ continue linéaire sur $X$. Nous utiliserons la notation de \emph{dualité}, utilisée dans la théorie des distributions:
\begin{equation}
\langle T, x\rangle = T(x)
\end{equation}

Nous pouvons également définir la notation de l'espace des fonctions indéfiniment dérivables de support compact sur $[a,b]$ : $C_0^\infty (]a,b[)$. Nous considérons que tous les compacts seront dans $]0,1[$ à des fins pratiques, comme cet intervalle correspond aux limites de notre problème.\\
Nous pouvons maintenant introduire la notion d' \emph{espace dual} : c'est l'espace défini par les formes linéaires $C_0^\infty (]0,1[)$, que l'on notera $D'(]0,1[)$. Les éléments le composant sont des \emph{distributions} : toute fonction localement intégrable $f$ est associée à une distribution notée $\phi$ et définie par
\begin{equation}
\langle f, \phi \rangle = \int_0^1 f \phi
\end{equation}
Nous avons maintenant les bases pour introduire les \emph{dérivées au sens des distributions} : soit $T \in D'(]0,1[)$. Alors, $\forall k \in \N *$, $T^{(k)}$ est une distribution telle que 
\begin{equation}
\langle T^{(k)}, \phi \rangle = (-1)^k \langle T, \phi^{(k)} \rangle \espace \forall \phi \in C_0^\infty(]0,1[)
\end{equation}

Un court exemple est disponible dans Alfio Quarteroni.

\subsection{Propriétés de la méthode de Galerkin}

La méthode de Galerkin permet la résolution du problème aux limites par une autre approche que les différences finies: on utilise ici la résolution du problème \eqref{problemeSecondaire}, appellée \emph{formulation faible}.\\
En supposant $V_h$ un sous espace vectoriel de $V$ tel que $dim(V_h) < \infty$, la méthode de Galerkin consiste à approximer le problème \eqref{problemeSecondaire} par:
\begin{equation} \label{problemeApproximé}
u_h \in V_h \espace | \espace a(u_h, v_h) = (f(v_h) \espace \forall v_h \in V_h
\end{equation}
Qui devient un autre problème lorsqu'on cherche $u_h$.
Il n'est pas forcément évident que ce problème soit en dimension finie (EXPLIQUER PQ?), nous allons le montrer. Soit $\{\phi_1, ..., \phi_N\}$ base de $V_h$, avec $dim(V_h) = N$. On peut alors dire que 
\begin{equation}
u_h(x) = \sum_{j = 1}^{N} u_j \phi_j(x)
\end{equation}
Avec $v_h = \phi_i$ dans \eqref{problemeApproximé}, le problème devient la recherche de $N$ réels $\{u_1, ..., u_N\} \espace |$
\begin{equation}
\sum_{j=1}^N u_j a(\phi_j, \phi_i) \espace \forall i \in [[1,N]]
\end{equation}
On rappellera que $a(.,.)$ est linéaire par rapport à la première place.
Nous introduirons la matrice $A_G = (a_{ij}) \espace | \espace a_{ij} = a(\phi_j, \phi_i)$, un vecteur colonne inconnu solution de notre ptoblème $u = [u_1, ..., u_N]$ et le \emph{second membre} colonne $f_G = [f_1, ..., f_N] \espace | \espace f_i = (f, \phi_i)$ : nous pouvons alors voir que comme pour \eqref{turboCompact}, nous pouvons compacter l'équation précédente en 
\begin{equation}
A_G u = f_G
\end{equation}
La précision de $u_h$ dépend de la forme des fonctions de la base $\phi$, qui est elle même déterminée par $V_h$.

Nous pourrions démontrer par la suite que la méthode de Galerkin est bornée par rapport à la dimension de $V_h$,  ce qui montre sa stabilité.

\subsection{Méthode des éléments finis}
Mettre les élts finis pour la résolution de Galerkin? ou osef?








\section{Introduction à la résolution du problème en dimension 2}


\section{Applications}

\subsection{Conduction de la chaleur}

\subsection{Déformation d'une corde élastique}
Dans le quarteroni page 433

\subsection{Problème stationnaire elliptique}

\subsection{Problème hyperbolique}



\end{document}