
% Le template fourni a été utilisé pour rédiger cet article.


\documentclass[12pt]{article}
\usepackage[utf8]{inputenc} 
%\usepackage[francais]{babel}

\usepackage{breqn}
%Dimensions
\usepackage{geometry }
\geometry{left=2cm,right=2cm,top=3.3cm,bottom=3.3cm}

% AMS packages:
\usepackage{amsmath, amsthm, amsfonts}

% Theoremess
%-----------------------------------------------------------------


\newtheorem{theoreme}{Th{\'e}or{\`e}me}[section]
\newtheorem{theoremedefinition}[theoreme]{Th{\'e}or{\`e}me et d{\'e}finition}
\newtheorem{definitionproposition}[theoreme]{Définition et Proposition}
\newcommand{\theoremeautorefname}{Th{\'e}or{\`e}me}
\newtheorem{proposition}[theoreme]{Proposition}%[section]
\newcommand{\propositionautorefname}{Proposition}
\newtheorem{lemme}[theoreme]{Lemme}%[section]
\newcommand{\lemmeautorefname}{Lemme}
\newtheorem{corollaire}[theoreme]{Corollaire}%[section]
\newcommand{\corollaireautorefname}{Corollaire}
\newtheorem{definition}[theoreme]{D\'efinition} %[section]
\newcommand{\definitionautorefname}{D\'efinition}
\newtheorem{definitions}[theoreme]{D\'efinitions}%[section]
\newtheorem{exemple}[theoreme]{Exemple}%[section]
\newtheorem{exemples}[theoreme]{Exemples}%[section]
\newtheorem{remarque}[theoreme]{Remarque}%[section]
\newcommand{\remarqueautorefname}{Remarque}
\newtheorem{remarques}[theoreme]{Remarques}%[section]
\newtheorem{probleme}[theoreme]{Probl{\`e}me}%[section]
\newtheorem{exercice}[theoreme]{Exercice}%[section]


% Shortcuts.
% One can define new commands to shorten frequently used
% constructions. As an example, this defines the R and Z used
% for the real and integer numbers.
%-----------------------------------------------------------------
\newcommand{\R}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\espace}{\hspace{5mm}}

% Similarly, one can define commands that take arguments. In this
% example we define a command for the absolute value.
% -----------------------------------------------------------------
\newcommand{\abs}[1]{\left\vert#1\right\vert}

% Operators
% New operators must defined as such to have them typeset
% correctly. As an example we define the Jacobian:
% -----------------------------------------------------------------
\DeclareMathOperator{\Jac}{Jac}

%-----------------------------------------------------------------
\title{Problèmes aux limites en dimension 1} % votre titre
\author{Simon Hergott} % votre Prénom Nom


\begin{document}
	\maketitle
	
\section{Introduction au problème}

\quad Cet article aura pour but de résumer les résultats et applications concernant les problèmes aux limites en dimension 1. Nous reviendrons sur la méthode des différences finies, fondant l'approximation des solutions des problèmes aux limites, et nous traiterons de plus quelques applications les plus courantes pour cette catégorie de problèmes. Éventuellement, nous verrons brièvement les méthodes de résolution de ces problèmes en dimension 2.
Rappellons d'abord l'énoncé du modèle du problème aux limites en dimension 1:
\begin{equation}\label{probleme}
-u''(x) = f(x) \hspace{1cm} \forall x \in ]0,1[ 
\end{equation}
\begin{equation}\label{limites}
u(0) = u(1) = 0
\end{equation}
\quad Ce problème est donc simplement caractérisé par l'équation \eqref{probleme} munie des conditions limites \eqref{limites}. Une partie importante lors de la résolution de ce problème est donc de trouver les valeurs propres et les fonctions propres (fonctions ne subissant qu'une transformation scalaire lors de leur utilisation comme solution) de l'équation différentielle
\begin{equation}
	X'' + \alpha X = 0 \hspace{1cm} \forall X \in ]0,1[ , \forall \alpha \in \mathbb R*
\end{equation}

\quad Nous pouvons alors écrire les solutions de \eqref{probleme} comme les fonctions de la forme
\begin{equation}
\label{solutionsNaives}
	u\relax(x) = c_1 + c_2 x - \int_0^x F(s) ds \espace c_1 , c_2 \in \R
\end{equation}
selon le théorème fondamental de l'Analyse, avec 
\begin{equation}
	F\relax(s) = \int_0^s f(t) dt
\end{equation}
\quad Nous verrons que l'on peut écrire $u$ sous la forme
\begin{equation}
	u\relax(x) = \int_0^1 G\relax(x, s) f\relax(s) ds \espace x \in [0, 1]
\end{equation}
ce qui nous permettra d'introduire avec $G$ le concept de \emph{Fonction de Greene}.

Les applications du problème aux limites en dimension 1 sont multiples, et relativement nombreuses dans la physique où sa résolution permet la simulation de plusieurs phénomènes tels que la conduction de la chaleur, et les défomrations élastiques.



\section{Méthode des différences finies}

\quad Développée en 1715 par Brook Taylor dans son ouvrage \emph{Methodus incrementorum directa et inversa}, la méthode des diffrences finies est un procédé courant utilisé pour approcher la solution d' équations différentielles. En résumé, on établit une grille de points généralement uniforme sur l'espace de recherche pour discrétiser le problème (le réduire à un nombre fini de pas), puis on réduit la distance entre les points pour approcher au maximum la solution.
\quad Formellement, on utilise la formule de Taylor pour discrétiser les différentielles n-ièmes : on peut alors choisir la formule de Taylor-Young, ou la formule de Taylor avec reste intégral pour évaluer les erreurs (la discrétisation induit une approximation, qui engendre des erreurs).
Dans le cas de Taylor-Young simple (on appellera ici de cette manière la formule de Taylor sans reste intégral), on a:
$$
f\relax(x_0 + h) = f\relax(x_0) + \sum_{\substack{0<i\le n}}\frac{f^{(i)}\relax(x_0)}{i!}h^i
$$ 

\quad La méthode des différences finies sert à approximer la dérivée de certaines fonctions par des valeurs numériques, qui ne peut parfois pas être calculées par des méthodes formelles classiques. En effet, généralement les fonctions ne peuvent pas être formulées analytiquement ce qui rend le calcul de leurs dérivées impossible.
\\
\quad Pour appliquer la méthode des différences finies, on se place sur un segment $[0, \alpha]$ (on verra plus tard que dans le cas du problème aux limites en dimension 1, $\alpha = 1$)sur lequel on ajoute un pas de discrétisation $h$ et une suite de points $x_n | n\in [0, N]$ avec $Nh = \alpha$. Ici, le pas est uniforme mais il peut tout à fait être défini non uniformément sur chaque $x_i$ pour tout $i \in [0, N-1]$ comme $h_i$, avec $\sum_{\substack{0 \le i< N}}h_i = \alpha$. On appellera $x_n$ la \emph{grille}.
\\
\quad Posons une fonction régulière $u$ telle que: $u : [0, \alpha] \longrightarrow \R$, et on appellera $u_n$ l'évaluation de $u$ en chaque point $x_n$ de la grille. \\
On appelle \emph{une différence finie à p points} une combinaison linéaire de $p$ $u_n$, servant à approximer au point $x_n$ les dérivées de $u$. Considérons $Du$ une différence finie, on dira qu'elle \emph{approche $u^{(l)}\relax(x_n)$ à l'ordre q} si :
\begin{equation}
\exists C>0, h_0 >0 \hspace{5mm}|\hspace{5mm} \forall h \in [0, h_0] \hspace{5mm} ||Du - u^{(l)}\relax(x_n)|| \le Ch^q
\end{equation}


%différences finies : voir alfredo quarteroni page 360



\quad Cette méthode permet donc de décomposer les équations différentielles ordinaires en un système d'équations linéaires, solvable en utilisant l'algèbre linéaire.




\section{Résolution du problème en dimension 1}

\quad Revenons à l'introduction de cet article : nous avions vu que pour $u \in C^2 [0, 1]$ satisfaisant l'équation \eqref{probleme}, on avait $u$ de la forme \eqref{solutionsNaives}  Pour retrouver la fonction de Greene, nous devons intégrer par parties $\int_0^x F\relax(s) ds$ : 
\begin{equation}
\int_0^x F\relax(s) ds = [sF\relax(s)]_0^x - \int_0^x sF'\relax(s) ds = \int_0^x(x-s)f\relax(s)ds
\end{equation}

D'après \eqref{limites}, les constantes $c_1$ et $c_2$ sont respectivement égales à $0$ et $\int_0^1(1-s)f\relax(s)ds$.  On peut alors écrire $u$ sous la forme
\begin{equation} \label{introGreene}
u\relax(x) = x \int_0^1(1-s)f\relax(s)ds - \int_0^x (x-s)f\relax(s)ds = \int_0^1 G\relax(x, s) f\relax(s) ds
\end{equation}
avec 
\begin{equation}
G\relax(x, s) = \left\{
    \begin{array}{ll}
        s(1-x) \espace s\in [0,x] \\
        x(1-s) \espace s\in [x, 1]
    \end{array}
\right.
\end{equation}
On remarquera qu'on a continuité de la fonction $G$ pour $s = x$, et que G est une fonction \emph{affine} de $x$ à $s$ fixé, et de $s$ à $x$ fixé. Elle se nomme \emph{Fonction de Greene} pour le problème aux limites défini par \eqref{probleme} et \eqref{limites}.

\subsection{L'histoire avec Greene}
\quad La fonction de Greene est continue et symétrique sur $[0,1]^2$, ainsi que positive non strictement ($G(x,s) = 0 \Longleftrightarrow x = 0 \lor s = 0$). Son existence dans le problème générique \eqref{probleme} (nous verrons plus tard des applications précises du problème comme la déformation d'une corde élastique) est assurée par sa définition sur tout l'intervalle entre les limites (dans notre cas, \eqref{limites} mais on pourrait fixer d'autres limites). \\
\quad On voit alors que lorsque $G$ existe et qu'elle est connue formellement, on peut écrire explicitement les soutions du problème aux limites dans une forme très simple.  Un des principaux avantages de la représentation \eqref{introGreene} des solutions du problème est qu'elle élimine la dépendance au terme $f(s)$, qui n'est dépendant que de l'équation différentielle en \eqref{probleme} et des limites qui nous sont imposées : une fois que l'on aura déterminé $G$, les solutions seront connues selon $f(s)$ pour peu pqu'on puisse écrire les solutions sous la forme \eqref{introGreene}. Nous verrons dans les applications que cette forme n'est pas forcément admissible sous toutes les conditions. \\
\quad De plus, la forme intégrale \eqref{introGreene} est bien plus propice à l'analyse numérique que l'équation différentielle \eqref{probleme}, ce qui rend le traitement par ordinateur bien plus simple et efficace. Les motivations principales sont donc de passer d'une recherche de $u$ à une recherche de $G$ : il faut alors trouver un moyen d'exprimer $G$ sans avoir à résoudre l'équation \eqref{probleme}.\\
Nous pouvons alors lister quelques propriétés de la fonction de Greene, qui nous seront utiles par la suite dans la résolution du problème:
\begin{enumerate}
  \item $G$ satisfait l'équation homogène de  \eqref{probleme}:
	\begin{equation}
		G'' = 0
	\end{equation}
	sur les intervalles $0 \le s < x$ et $x < s \le L$ avec dans notre cas L = 1. Généralement, on a bien continuité en $s = x$, nous le prouverons dans le cas du problème de l'élasticité d'une corde. (ou pas? Remettre la démo éventuellement)
  \item $G(x, 0) = G(x, L)$, elle satisfait donc les conditions limites \eqref{limites}.
  \item $G(x,s) = G(s, x)$, G est symétrique sur ses arguments.
\end{enumerate}

A partir de ces propriétés, nous pouvons commencer la résolution du problème aux limites en supposant l'existence d'une fonction de Greene $G$. Si cette supposition est valide, alors nous pouvons retrouver \eqref{introGreene} depuis \eqref {probleme}, ainsi que ses propriétés listées précédemment.  En effet,  d'après \eqref{probleme}, après une multiplication des deux côtés par $G$ nous pouvons intégrer comme suit:
\begin{equation}
\int_0^1 u'' G(x,s) dx = - \int_0^1 f(x) G(x,s) dx
\end{equation}
avec comme seule supposition la continuité lorsque $s \rightarrow x$. Pour être plus formel, nous pouvons exclure de l'intervalle d'intégration le point $x = s$ et éviter toute intégrale impropre.
\begin{equation} \label{separation}
\int_0^1 u'' G(x,s) dx = \lim_{\epsilon \to s^- } \int_0^\epsilon u'' G(x,s) dx + \lim_{\eta \to s^+ } \int_\eta^1 u'' G(x,s) dx
\end{equation}

En intégrant deux fois par parties les deux intégrales à droite,  nous avons:
\begin{equation}
\int_0^\epsilon u'' G(x,s) dx = [Gu' - G'u]_0^\epsilon + \int_0^\epsilon u G''(x,s) dx
\end{equation}
\begin{equation}
\int_\eta^1 u'' G(x,s) dx = [Gu' - G'u]_0^\epsilon + \int_\eta^1 u G''(x,s) dx
\end{equation}
Or, en choisissant $G(x,s)$ de manière à satisfaire $G'' = 0$ en tant que fonction de x dans les intervalles des intégrales ($[0, \epsilon] \cup [\eta, 1]$), alors les intégrales à droite sont nulles. En ajoutant les termes restants (qu'on avait enlevés afin d'éviter les intégrales impropres), on revient en supposant $G$ symétrique à:
\begin{dmath}
-\int_0^1 f(x)G(x, s) dx = (G(s, s^- ) u'(s^-) - G'(s, s^- ) u(s^-) - G(0, s ) u'(0) + G(1, s ) u'(1) - G(s, s^+ ) u'(s^+) + G'(s, s^+ ) u(s^+))
\end{dmath}

En supposant que $G$ vérifie les conditions limites et que $u$ est continu en $s$,  nous pouvons écrire: 
\begin{equation} \label{dernierRempartDeLEncadre}
\int_0^1 f(x)G(x, s) dx = - u(y) (G'(s, s^+ ) - G'(s, s^- ))  + u'(s)( G(s, s^+ ) - G(s, s^- ))
\end{equation}


%METTRE TOUT CA DANS UN ENCADRÉ
De plus,  $G$ est continue en $x=s$, tandis que $G'$ y est discontinue. Nous aurons besoin de cette proposition pour continuer, nous allons la démontrer. \\
À partir de \eqref{probleme}, nous pouvons utiliser la méthode de la variation des variables pour supposer que, si le problème existe, alors les solutions seront de la forme 
\begin{equation} \label{solutionBasique}
u(x) = A(x) cos(kx) + B(x) sin(kx).
\end{equation}
En dérivant deux fois selon x et en supposant qu'on ait $A'(x) cos (kx) + B'(x) sin(kx) = 0$, on trouve que \eqref{solutionBasique} constitue effectivement une solution, à la condition
\begin{equation} \label{resoudre}
-kA' sin(kx) + kB' cos(kx) = -f(x)
\end{equation}
En résolvant \eqref{resoudre} accompagné de sa condition présupposée, nous pouvons alors exprimer $A'$ et $B'$:
\begin{equation}
A'(x) = \frac{f(x) sin(kx)}{k}
\end{equation}
\begin{equation}
B'(x) = \frac{-f(x) cos(kx)}{k}
\end{equation}

Nous pouvons alors écrire la solution de \eqref{probleme} comme:
\begin{equation}
u(x) = \frac{cos(kx)}{k} \int_{c_1}^x f(s) sin (ks) ds - \frac{sin(kx)}{k} \int_{c_2}^x f(s) cos (ks) ds
\end{equation}
avec $c_1, c_2$ constantes bien choisies pour satisfaire les conditions \eqref{limites}. En utilisant la condition en 0 de \eqref{limites}, on trouve $c_1 = 0$. De même, la condition en 1 implique
\begin{equation}
u(x) = \frac{1}{k} \int_0^x f(s) sin (k(s-x)) ds - \frac{sin(kx)}{k sin(kl)} \int_x^1 f(s) sin(k(s-1) ds
\end{equation}
ce qui nous donne enfin
\begin{dmath}
u(x) =  \int_0^x\frac{sin(ks) sin(k(1-x))}{k sin(k)}ds + \int_x^1 f(s) \frac{sin(kx) sin(k(1-s))}{k sin(kl)} ds = \int_0^1 f(s) G(x,s) ds
\end{dmath}
qui revient au même pour introduire la fonction de Greene, de manière bien détaillée cette fois. On a par la même occasion montré la consistance du problème aux limites (revoir la def!!!!). \\
Finalement, nous aboutissons à une forme de $G$ intéressante:
\begin{equation}
G(x, s) = \frac{sin(ky) sin(k(1-x)}{k sin(k)} \espace s\in [0, x] 
\end{equation}
\begin{equation}
G(x, s) = \frac{sin(kx) sin(k(1-s)}{k sin(k)} \espace s\in [x,1]
\end{equation}

À partir d'ici, nous pouvons vérifier aisément que la fonction $G$ est continue en $s=x$, et que sa dérivée est discontinue en ce même point.
%FIN DE L'ENCADRÉ
\\
Depuis \eqref{dernierRempartDeLEncadre}, nous pouvons alors conclure que
\begin{equation}
u(s) = \int_0^1 f(x)G(x, s) ds
\end{equation}
et nous obtenons alors une représentation désirable de la solution duproblème aux limites par une fonction $G$. Cette fonction vérifie tous les axiomes de la fonction de Greene énoncés auparavant, nous avons donc établi une méthode pour retrouver la solution des problèmes aux limites d'une manière relativement directe. Néanmoins, cette méthode suppose que nous connaissions la fonction $G$.

\subsection{Suite de la resolution : approximation par différences finies}

Sur la grille des points $(x_j)_{j = 0}^n$ de pas uniforme $h$ (on a donc $x_j = jh$) sur $[0,1]$, l'approximation de la solution est une suite finie $(u_j)_{j = 0]}^n$ telle que:
\begin{equation} \label{diffsFiniesProbleme}
	- \frac{u_{j+1} - 2u_j + u_{j-1}}{h^2} = f(x_j) \espace j \in [[1, n-1]]
\end{equation} 
avec $u_i = 0$ aux points $0$ et $n$. On a alors $u_j$ approche $u(x_j)$, la valeur de chaque point de la grille. Pour s'en convaincre, on se réfèrera à ... dans la partie traitent des différences finies centrées, en remplaçant $u''(x)$ par son approximation du 2nd ordre.

On posera $u = <u_1, ..., u_n-1>$ et $f = <f_1, ..., f_n-1>$ vecteurs avec $f_i = f(x_i)$, en utilisant une notation empruntée au C++. Nous pouvons alors voir que \eqref{diffsFiniesProbleme} peut s'écrire comme
\begin{equation} \label{turboCompact}
	A_{df}u = f
\end{equation}
avec $A_{df}$ matrice carrée de différences finies de taille $(n-1)$ définie par
\begin{equation}
	A_{fd} = h^{-2} tridiag_{n-1} (-1, 2, -1)
\end{equation}
Elle est à diagonale dominante par ligne, et définie positive (démontrable).

Alors, l'équation \eqref{turboCompact} n'admet qu'une unique solution.\\ Définissons les M-Matrices comme suit : une M-Matice est une matrice carrée inversible avec tous ses coefficients non sur la diagonale négatifs ou nuls, ainsi que tous les coefficients de son inverse sont positifs ou nuls.
Nous pouvons noter que $A_{df}$ est une M-Matrice, ce qui permet de satisfaire la condition de monotonie de la solution exacte $u(x)$ : nous avons alors $f >0 \Longrightarrow u>0$ (propriété du \emph{maximum discret}).

Le but est maintenant de réécrire \eqref{diffsFiniesProbleme}. Pour cela, nous pouvons considérer $V_h$ ensemble de fonctions discrètes définies sur les $x_j$ points de la grille. Pour tout $v_h$ dans $V_h$, elle est alors définie en tout point de la grille, et $v_j = v_h(x_j)$.\\ On posera de plus $V_h^0 = \{v_h \in V_h | v_0 = v_n = 0\}$.
On pourra définir $L_h$ comme
\begin{equation}
	(L_h v_h)(x_j) = - \frac{v_{j+1} - 2v_j + w_{j-1}}{h^2} \espace \espace v_h \in V_h \espace j \in [[1, n-1]]
\end{equation}
On peut alors encore réécrire le problème \eqref{diffsFiniesProbleme} comme:
\begin{equation}
	(L_h u_h)(x_j) = f(x_j) \espace j \in [[1, n-1]]
\end{equation}
en cherchant $u_h \in V_h^0$ : on prend en compte les conditions limites.

Remettre ici la suite de la méthode des différences finies, ou la redéfinir dans la section précédente.

Nous allos ensuite analyser la stabilité du problème, en utilisant la méthod ede l'énergie (stabilité : capacité de l'algo à ne pas amplifier les erreurs et à produire des résultats cohérents, mettre la def). On cherche à montrer que la solution est bornée en partant d'une valeur initiale du problème, et que la méthode des différences finies permettra de s'ens approcher

Continuer avec l'analyse de la convergence des solutions : regarder dans un autre bouquin pour changer?

Si place, regarder les différences finies pour la résolution de problèmes à coefs variables; sinon, on s'en tape.










\section{Introduction à la résolution du problème en dimension 2}


\section{Applications}

\subsection{Conduction de la chaleur}

\subsection{Déformation d'une corde élastique}

\subsection{Problème stationnaire elliptique}

\subsection{Problème hyperbolique}



\end{document}